{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b4a197-b8a2-4d15-9018-51f10af68f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Write a program in Python, to read the Tweet Dataset from MongoDB. For each tweet, \n",
    "#extract words from its ‘text’ field and print the words and the category of words in Python \n",
    "#console. (Note that, in order to extract words, you’d better remove stop words and \n",
    "#derivational affixes in the texts. Here, we can use a natural language toolkit (nltk) to handle \n",
    "#these issues. For POS, use nltk.pos.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6106ffd0-72ce-473c-8cbb-85a1f90d64d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient  \n",
    "import nltk  \n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.stem import PorterStemmer  \n",
    "from nltk.corpus import stopwords  \n",
    "import re  \n",
    "\n",
    "def download_nltk() -> None:  \n",
    "\"\"\"need to download these once before you can run any of their related methods \n",
    "on a computer\"\"\"  \n",
    "    nltk.download(\"stopwords\") #used for stopwords  \n",
    "    nltk.download(\"punkt\") #used for tonkenising  \n",
    "    nltk.download('averaged_perceptron_tagger') #used for Parts-of-Speech tagging  \n",
    "\n",
    "def remove_stopwords(words: list) -> list:  \n",
    "\"\"\"returns list of words without inconsequetial/unimportant/common words\"\"\"  \n",
    "    result: list = []  \n",
    "    for w in words:  \n",
    "        if w not in stopwords.words(\"english\"):  \n",
    "            result.append(w)  \n",
    "    return result  \n",
    "\n",
    "def stem_words(words: list) -> list:  \n",
    "\"\"\"reduces words to their root word by removing derivational affixes\"\"\"  \n",
    "    ps: PorterStemmer = PorterStemmer()  \n",
    "    result: list = []  \n",
    "    for w in words:  \n",
    "        result.append(ps.stem(w))  \n",
    "    return result  \n",
    "\n",
    "def find_keywords_in_text(text: str) -> list:  \n",
    "    text = re.sub(r\"[^\\w]\",\" \", text) #removes non-alphabetical symbols (like punctuation and numbers)  \n",
    "    words: list = word_tokenize(text) #convert into list of words  \n",
    "    words = remove_stopwords(words) #remove stopwords  \n",
    "    words = stem_words(words) #make the words undergo a form of linguistic normalisation  \n",
    "    keywords: list = nltk.pos_tag(words) #run Parts-of-Speech tagging to identify grammatical groupings of keywords  \n",
    "    return keywords  \n",
    "\n",
    "#accesses database and its collection 'Tweets'  \n",
    "client: MongoClient = MongoClient('127.0.0.1',27017)  \n",
    "db = client[\"myNewDB\"]  \n",
    "tweets = db[\"Tweets\"]  \n",
    "download_nltk() #only necessary on first time the script is run, left in to \n",
    "demonstrate \n",
    "#loops through each tweet, calls various functions to help in extracting keywords,  \n",
    "#then updates the database with the keywords in CSV format  \n",
    "for t in tweets.find():  \n",
    "    keywords = find_keywords_in_text(t[\"text\"]) \n",
    "    for k in keywords: \n",
    "        print(k) \n",
    "    print(\"\\n\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagemaker-distribution:Python",
   "language": "python",
   "name": "conda-env-sagemaker-distribution-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
